{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment simulations using ODEs\n",
    "\n",
    "Environment dynamics with agents with some understanding of how the environment\n",
    "works can be simulated as a system of ordinary differential equations. Below is\n",
    "an example of how to run simulations like that using `gxr.envir` package.\n",
    "\n",
    "For now (this will be extended later) our prototype analysis focuses on the effects\n",
    "of:\n",
    "\n",
    "* Time horizon in which agents try to predict consequences of their actions,\n",
    "  i.e. the effects of how much resource they extract from the environment.\n",
    "  This is controlled by `horizon` parameter, which defines (approximately) the\n",
    "  length of the time horizon of foresight expressed in terms of the characteristic\n",
    "  timescale of the environment (the amount of time it needs to regenerate from 5%\n",
    "  to 95% of its carrying capacity).\n",
    "  * `horizon` $\\in (0, \\infty)$\n",
    "  \n",
    "* For now the default number of agents is ``4`` as this is the planned number\n",
    "  of participants in VR experiments.\n",
    "\n",
    "There are of course several other control parameters, but for now they are all set\n",
    "to reasonable defaults, and therefore we are not interested in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a single simulation\n",
    "\n",
    "Below is an example of how to run a single simulation and visualize its results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gxr.envir import Config, DynamicsPlotter, EnvirDynamics, EnvirModel\n",
    "\n",
    "# Configuration for the game; only value to override must be provided;\n",
    "# all other options are set to reasonable defaults.\n",
    "# Use '.from_disk()' method to populate a config with value from a '.toml' file.\n",
    "params = {\n",
    "    \"foresight\": {\n",
    "        \"horizon\": .01,\n",
    "    },\n",
    "}\n",
    "config   = Config(params)\n",
    "model    = EnvirModel(**config[\"model\"])\n",
    "dynamics = EnvirDynamics(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `dynamics` object to run a simulation of environment-agents dynamics.\n",
    "The first argument is the length of the run expressed in terms of the characteristic\n",
    "timescale.\n",
    "\n",
    "The solution object, `sol`, exposes a `.get_arrays()` method returning a names tuple with 4 arrays:\n",
    "\n",
    "1. ``T`` - Time grid.\n",
    "2. ``E`` - Environment states in time.\n",
    "3. ``P`` - Agents profits in time, that is, the cumulative resources extracted \n",
    "           from the minus the sustenance costs.\n",
    "4. ``H`` - Individual harvesting rates of agents in time. These are the intensities\n",
    "           of resource extraction (and the actual gain is proportional to the\n",
    "           harvesting intensity and the current state of the environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol  = dynamics.run(30, progress=True)\n",
    "T, E, P, H = sol.get_arrays()\n",
    "\n",
    "# Solver message\n",
    "sol.ode.message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE.** Sometimes solver may not reach the end of the integration interval.\n",
    "This happens almost exclusively when the environment state becomes low enough,\n",
    "to be dominated by floating point errors.\n",
    "\n",
    "It is probably best to rerun simulations for which ``sol.ode.success is not True``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also quite easy to visualize the results of a single simulation run using\n",
    "out `DynamicsPlotter` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = DynamicsPlotter(dynamics, sol)\n",
    "fig, axes = plotter.subplots(nrows=3, figsize=(8, 8))\n",
    "\n",
    "plotter.plot_state(axes[0], show_vicious=True, show_perceived=True)\n",
    "plotter.plot_harvesting(axes[1])\n",
    "plotter.plot_utilities(axes[2])\n",
    "fig.tight_layout()\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE.** Currently, an identity utility function is used, so utilities are\n",
    "equal to profits (i.e. extracted resource minus sustenance cost)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running multiple simulations for generating training data\n",
    "\n",
    "Here we discuss an example of how the problem of using simulations to generate\n",
    "training data can be approached. It is of course, is not the only possible solution,\n",
    "but rather a guide for how other custom approaches can be designed.\n",
    "\n",
    "For now, we focus on effects of the `horizon` parameter, so we will study over\n",
    "some grid from `.01` to `20`. For each value we generate `n_reps` instances of\n",
    "dynamics and save the results using the mean final profit of agents as the target\n",
    "variable that, in principle, should be maximized by a learned policy.\n",
    "\n",
    "Currently, there is no easy way of modifying `horizon` (or any other control\n",
    "parameter for that matter) during a simulation, so it is not clear generate data\n",
    "from which effects of interventions (i.e. changes of `horizon`) could be inferred.\n",
    "However, this problem can be bypassed using a trick.\n",
    "\n",
    "For each instance of dynamics we will sample one (or more) sub-trajectories,\n",
    "i.e. from time $t_1$ to $t_2$, with lengths selected uniformly at random.\n",
    "And since the dynamics have a form of a continuous time Markov chain, each such\n",
    "sample will describe an effect of a having a particular value of `horizon`\n",
    "conditional on a particular set of initial conditions \n",
    "(environment state and current agents' profits and individual harvesting rates)\n",
    "over a particular time interval.\n",
    "\n",
    "And it is showed below, it is quite easy to generate data like that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "## FIRST DEFINE SOME BASIC PARAMETERS OF THE SIMULATION\n",
    "## The values provided here are of course only examples.\n",
    "\n",
    "HORIZON   = np.linspace(.01, 10, 20)\n",
    "T_MAX     = 50  # simulation run time in terms of the characteristic timescale\n",
    "N_REPS    = 10   # number of simulation runs per a parameter configuration\n",
    "N_SAMPLES = 10   # number of samples from a single simulation run\n",
    "\n",
    "## Set random seed for sampling reproducibility\n",
    "random.seed(1010103)\n",
    "\n",
    "results = []\n",
    "\n",
    "for horizon in tqdm(HORIZON):\n",
    "    for _ in tqdm(range(N_REPS), leave=False):\n",
    "        params = {\"foresight\": {\"horizon\": horizon}}\n",
    "        config   = Config(params)\n",
    "        model    = EnvirModel(**config[\"model\"])\n",
    "        dynamics = EnvirDynamics(model)\n",
    "\n",
    "        sol = dynamics.run(T_MAX, progress=False)\n",
    "        result = {\n",
    "            \"horizon\": horizon,\n",
    "            \"data\": sol.get_arrays()\n",
    "        }\n",
    "\n",
    "        for _ in range(N_SAMPLES):\n",
    "            ## Here we get a sample and make time and profit values relative,\n",
    "            ## i.e. starting time and profits are 0.\n",
    "            sample = sol.get_arrays(sample=True).relative\n",
    "            result.setdefault(\"samples\", []).append(sample)\n",
    "\n",
    "        results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data representation can be easily turned, for instance, into `pandas` data frames\n",
    "allowing for easy computation of target values, aggregation and visualization.\n",
    "\n",
    "Below is a graph of target values with respect to horizon lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "sim = (\n",
    "    pd.DataFrame(results)\n",
    "    .assign(target=lambda df: df[\"data\"].map(lambda A: A.P[..., -1].mean()))\n",
    ")\n",
    "\n",
    "data = sim\n",
    "mean_target = data.groupby(\"horizon\")[\"target\"].mean()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(mean_target.index, mean_target, color=\"red\", label=\"mean target value\")\n",
    "ax.scatter(data[\"horizon\"], data[\"target\"])\n",
    "ax.set_xlabel(\"Time horizon\")\n",
    "ax.set_ylabel(\"Target (mean final profit)\")\n",
    "ax.legend()\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation between `horizon` and final average profits is much weaker, but this\n",
    "is due to the fact that samples are of different lengths and have different starting\n",
    "conditions.\n",
    "\n",
    "Crucially, an individual sample provides information on a target value after a given\n",
    "time starting from initial conditions, that is, environment state and agents' profits\n",
    "and harvesting rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pd.DataFrame({\n",
    "    \"horizon\": sim[\"horizon\"],\n",
    "    \"time\": sim[\"samples\"].map(lambda S: [s.T.max() for s in S]),\n",
    "    \"target\": sim[\"samples\"].map(lambda S: [s.P[..., -1].mean() for s in S])\n",
    "}).explode([\"time\", \"target\"])\n",
    "\n",
    "data = samples\n",
    "# data = data[data[\"time\"] > 5000]\n",
    "mean_target = data.groupby(\"horizon\")[\"target\"].mean()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(mean_target.index, mean_target, color=\"red\", label=\"mean target value\")\n",
    "ax.scatter(data[\"horizon\"], data[\"target\"])\n",
    "ax.set_xlabel(\"Time horizon\")\n",
    "ax.set_ylabel(\"Target (mean final profit)\")\n",
    "ax.legend()\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a more detailed exposition of information a single sample contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa\n",
    "sample = results[0][\"samples\"][0]\n",
    "\n",
    "sample.T      # time grid of the sample starting from 0\n",
    "sample.T[-1]  # time interval length\n",
    "\n",
    "sample.E      # environment states in time\n",
    "\n",
    "sample.P                  # agents' profits in time\n",
    "sample.P[..., -1].mean()  # average final agents' profit aka target\n",
    "\n",
    "sample.H  # agents' individual harvesting rates in time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle, the information listed above may be used for learning the relation between\n",
    "`horizon`, initial conditions and expected target values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some caveats\n",
    "\n",
    "* In this approach the learning problem actually reduces to a supervised scenario.\n",
    "* Samples taken from the same dynamics will be somehow correlated. It is hard to know\n",
    "  for sure, but probably this should not be too much of a problem for learning as long\n",
    "  as the number of independent simulated dynamics is high enough."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
